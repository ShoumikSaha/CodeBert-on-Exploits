
from code_to_embedding import get_embedding_from_input
from code_processor import code_to_function
import numpy as np
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split

def create_dataset(codes_list_w_functions, file_names, hashes, cwes, cwe1, cwe2):
    pos_x = []
    neg_x = []
    pos_y = []
    neg_y = []
    for i, cwe in enumerate(list(cwes)):
        if cwe==cwe1:
            #print(hashes[i])
            for j, file_name in enumerate(file_names):
                file_name = str(file_name).split('.')[0]
                #print(file_name)
                if file_name==hashes[i]:
                    pos_x.append(codes_list_w_functions[j])
                    pos_y.append(1)
                    break
        elif cwe==cwe2:
            #print(hashes[i])
            for j, file_name in enumerate(file_names):
                file_name = str(file_name).split('.')[0]
                #print(file_name)
                if file_name==hashes[i]:
                    neg_x.append(codes_list_w_functions[j])
                    neg_y.append(0)
                    break
    return pos_x, neg_x, pos_y, neg_y

def create_dataset_multiclass(codes_list_w_functions, file_names, hashes, cwes, picked_cwes):
    x = []
    y = []
    for i, cwe in enumerate(list(cwes)):
        for j, picked_cwe in enumerate(list(picked_cwes)):
            if(picked_cwe==cwe):
                for k, file_name in enumerate(file_names):
                    file_name = str(file_name).split('.')[0]
                    # print(file_name)
                    if file_name == hashes[i]:
                        x.append(codes_list_w_functions[k])
                        y.append(j)
                        break
    return x, y

def get_embedding_for_dataset(model, tokenizer, pos_x, neg_x):
    pos_embeddings = []
    neg_embeddings = []
    for i, code in enumerate(pos_x):
        for j, function in enumerate(code):
            #print(function)
            emb = get_embedding_from_input(model, tokenizer, function)
            pos_embeddings.append(emb.detach().numpy())

    for i, code in enumerate(neg_x):
        for j, function in enumerate(code):
            #print(function)
            emb = get_embedding_from_input(model, tokenizer, function)
            neg_embeddings.append(emb.detach().numpy())

    return np.asarray(pos_embeddings), np.asarray(neg_embeddings)

def get_embedding_for_dataset_multiclass(model, tokenizer, x, y):
    embeddings = []
    labels = []
    for i, code in enumerate(x):
        for j, function in enumerate(code):
            emb = get_embedding_from_input(model, tokenizer, function)
            embeddings.append(emb.detach().numpy())
            labels.append(y[i])
    return np.asarray(embeddings), np.asarray(labels)

def train_test_linear_classifier(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)
    clf = make_pipeline(StandardScaler(), SGDClassifier(max_iter=1000, tol=1e-3))
    clf.fit(X_train, y_train)
    print(y_train, y_test)
    print(clf.score(X_train, y_train))
    print(clf.score(X_test, y_test))




