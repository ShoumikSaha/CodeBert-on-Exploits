import numpy as np
# Press ⌃R to execute it or replace it with your code.
# Press Double ⇧ to search everywhere for classes, files, tool windows, actions, and settings.

#import torch
from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, AutoTokenizer, AutoModel
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import json
import os
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns

from code_processor import code_to_function
from code_to_embedding import load_model_tokenizer, get_embedding_from_input
from json_reader import convert_json_file_to_list_of_functions
from embedding_plotter import twoD_plotter, PCA_plotter, get_tSNE_embedding, get_PCA_embedding
from poc_features import retrieve_hash_with_cwe, dict_with_count, retrieve_hash_with_author
from litmus_test import create_dataset, create_dataset_multiclass, get_embedding_for_dataset, get_embedding_for_dataset_multiclass, train_test_linear_classifier
from custom_model import custom_model


def read_json(file_name):
    """

    :param file_name: name of the json file
    :return: a list of json objects from a json file
    """
    docs = []
    with open(file_name) as f:
        for json_obj in f:
            data = json.loads(json_obj)
            docs.append(data)
    return docs


def retrieve_PL_codes(path, lang='py'):
    """

    :param path: path of a folder that contains all the program files
    :param lang: language of the programs
    :return: a 2d list containing all functions for each program file
    """
    os.chdir(path)
    codes_list_w_functions = []
    file_names = []
    for file in os.listdir():
        file_path = f"{path}/{file}"
        extension = (str(file)).split('.')[1]
        #print(extension)
        if (extension == lang):
            #print(extension)
            #print(file_path)
            data = read_json(file_path)
            code_list = (data[0]['noncomments'])
            if (len(code_list) > 0):
                # print(code_list)
                code_lines = ""
                # code_tokens = [tokenizer.cls_token]
                for code_line in code_list:
                    code_lines += code_line

                functions_list = code_to_function(code_lines)
                if(len(functions_list)<=0): continue
                codes_list_w_functions.append(functions_list)
                file_names.append(file)

    return codes_list_w_functions, file_names

def functions_info_writer(codes_list_w_functions, common_path, threshold=512):
    """
    Writes
    1) Number of functions
    2) Code of functions preceded by the length
    :param codes_list_w_functions:
    :param common_path:
    :param threshold: input length threshold
    :return:
    """
    function_count = 0
    function_within_threshold_count = 0

    for i, code in enumerate(codes_list_w_functions):
        file = open(common_path + file_names[i], "w")
        print(file_names[i])
        print("Number of functions: ", len(code))
        file.write("Number of functions: " + str(len(code)) + "\n")
        function_count += len(code)
        for j, function in enumerate(code):
            # print(j+1, ") Length of function: ", len(function))
            pl_tokens = tokenizer.tokenize(function)
            print(len(pl_tokens))
            if (len(pl_tokens) < (threshold-2)): function_within_threshold_count += 1
            file.write(str(len(pl_tokens)))
            file.write("\n")
            file.writelines(function)
        file.close()
    # print(codes_list_w_functions)

    print(function_count, function_within_threshold_count)

def embedding_writer(codes_list_w_functions):
    """
    Saves a numpy file for PoC embeddings
    :param codes_list_w_functions:
    :return:
    """
    POC_embeddings = []
    for i, code in enumerate(codes_list_w_functions):
        for j, function in enumerate(code):
            function_embedding = get_embedding_from_input(model, tokenizer, function)
            POC_embeddings.append(function_embedding.detach().numpy())
            # print(function_embedding.shape)
            # print(function_embedding)
        # if(i==5):  break
    POC_embeddings = np.array(POC_embeddings)
    print(POC_embeddings.shape)
    # print(POC_embeddings)
    file = open(
        "/Users/shoumik/Desktop/UMD 1st semester/Research/Code/CodeBert on Exploits/embeddings/poc_embedding.npy", 'wb')
    np.save(file, POC_embeddings)
    file.close()
    return POC_embeddings

def get_non_PoC_embedding_codesearchnet():
    """
    From Code Search Net 2 database, selects some Non-PoC code, and saves their embedding
    :return:
    """
    path = "/Users/shoumik/Desktop/UMD 1st semester/Research/Dataset/CodeSearchNet 2/python/test.jsonl"
    NON_POC_function_list = convert_json_file_to_list_of_functions(path)
    # print(len(NON_POC_function_list))
    # print(len(NON_POC_function_list[0]))
    NON_POC_embeddings = []
    for i, function in enumerate(NON_POC_function_list):
        function_embedding = get_embedding_from_input(model, tokenizer, function)
        NON_POC_embeddings.append(function_embedding.detach().numpy())
        if (i == 4500):   break
    NON_POC_embeddings = np.array(NON_POC_embeddings)
    print(NON_POC_embeddings.shape)
    file = open(
        "/Users/shoumik/Desktop/UMD 1st semester/Research/Code/CodeBert on Exploits/embeddings/non_poc_embedding.npy",
        'wb')
    np.save(file, NON_POC_embeddings)
    file.close()
    return NON_POC_embeddings

def create_PoC_vs_non_PoC_embedding_dataset(POC_embeddings, non_POC_embeddings):
    """
    Combines the PoC and Non-PoC embeddings with labels
    :param POC_embeddings:
    :param non_POC_embeddings:
    :return:
    """
    combined_embedding = np.concatenate((POC_embeddings, non_POC_embeddings))
    print(combined_embedding.shape)
    combined_embedding_CLS = combined_embedding[:, :, 0, :]   ## '0' in the 3rd dimesnion indicates the [CLS] embedding
    print(combined_embedding_CLS.shape)
    combined_embedding_CLS = np.reshape(combined_embedding_CLS,
                                        (combined_embedding_CLS.shape[0], combined_embedding_CLS.shape[2]))
    # combined_embedding = np.reshape(combined_embedding, (combined_embedding.shape[0], combined_embedding.shape[2]*combined_embedding.shape[3]))
    print(combined_embedding_CLS.shape)
    del POC_embeddings
    del non_POC_embeddings
    del combined_embedding

    y_PoC = np.ones(4475)
    y_non_PoC = np.zeros(4501)
    y = np.concatenate((y_PoC, y_non_PoC))
    print(y.shape)

    return combined_embedding_CLS, y


if __name__ == '__main__':

    model, tokenizer = load_model_tokenizer("microsoft/codebert-base")

    path = r"/Users/shoumik/Desktop/UMD 1st semester/Research/Dataset/copy20221006/files/train_19700101_20210401/exploits_text"
    common_path = "/Users/shoumik/Desktop/UMD 1st semester/Research/Dataset/copy20221006/files/train_19700101_20210401/exploits_function/"

    codes_list_w_functions, file_names = retrieve_PL_codes(path)
    print(len(codes_list_w_functions))
    print(len(file_names))


    common_path_embedding = "/Users/shoumik/Desktop/UMD 1st semester/Research/Code/CodeBert on Exploits/embeddings/"

    #POC_embeddings = np.load(common_path_embedding + "poc_embedding.npy")
    #print(POC_embeddings.shape)

    #non_POC_embeddings = np.load(common_path_embedding + "non_poc_embedding.npy")
    #print(non_POC_embeddings.shape)


    """
    PCA_embedding = np.load(common_path_embedding + 'PCA_embedding.npy')
    tsne_embedding = np.load(common_path_embedding + 'tSNE_embedding.npy')
    tSNE_plotter(tsne_embedding, y)
    PCA_plotter(PCA_embedding, y)
    """

    path = "/Users/shoumik/Desktop/UMD 1st semester/Research/Dataset/copy20221006/files/train_19700101_20210401/"

    py_hash_cwe_file_path = path + "py_cve_cwe.csv"
    hashes, cwes = retrieve_hash_with_cwe(py_hash_cwe_file_path)
    print(hashes, cwes)
    cwe_count_dict = dict_with_count(cwes)
    print(cwe_count_dict)

    """
    py_hash_author = path + "py_author.csv"
    hashes, authors = retrieve_hash_with_author(py_hash_author)
    print(hashes, authors)
    author_count_dict = dict_with_count(authors)
    print(author_count_dict)
    """

    #x, y = create_dataset_multiclass(codes_list_w_functions, file_names, hashes, cwes, ['CWE-78', 'CWE-89'])
    #x, y = create_dataset_multiclass(codes_list_w_functions, file_names, hashes, authors, [' His0k4', ' Abysssec', ' muts', ' mr_me', ' loneferret', ' shinnai', ' Chris Lyne'])
    #print(x, y)

    #file = open("/Users/shoumik/Desktop/UMD 1st semester/Research/Code/CodeBert on Exploits/embeddings/cwe78_embs.npy", 'wb')
    #np.save(file, pos_embs)
    #file = open("/Users/shoumik/Desktop/UMD 1st semester/Research/Code/CodeBert on Exploits/embeddings/cwe89_embs.npy", 'wb')
    #np.save(file, neg_embs)
    """
    embs_multiclass, y_list = get_embedding_for_dataset_multiclass(model, tokenizer, x, y)
    print(embs_multiclass, y_list)
    embs_multiclass = embs_multiclass[:, :, 0, :]   ## '0' in the 3rd dimesnion indicates the [CLS] embedding
    embs_multiclass = np.reshape(embs_multiclass, (embs_multiclass.shape[0], embs_multiclass.shape[2]))
    print(embs_multiclass.shape)
    """
    #tsne_embs = get_tSNE_embedding(2, combined_embs)
    #twoD_plotter(tsne_embs, y, "tSNE CWE-434 vs CWE-89")
    #train_test_linear_classifier(embs_multiclass, y_list)
    #del embs_multiclass

    #custom_model(x, y)

    CWES = [['CWE-434', 'CWE-89'], ['CWE-434', 'CWE-79'], ['CWE-287', 'CWE-787'], ['CWE-78', 'CWE-89']]
    for i, picked_cwe in enumerate(CWES):
        print("--------------------------")
        print(picked_cwe)
        x, y = create_dataset_multiclass(codes_list_w_functions, file_names, hashes, cwes, picked_cwe)
        print(x, y)
        custom_model(x, y, 15)


