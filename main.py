import numpy as np
# Press ⌃R to execute it or replace it with your code.
# Press Double ⇧ to search everywhere for classes, files, tool windows, actions, and settings.

#import torch
from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, AutoTokenizer, AutoModel
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import json
import os
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns

from code_processor import code_to_function
from code_to_embedding import load_model_tokenizer, get_embedding_from_input
from json_reader import convert_json_file_to_list_of_functions
from embedding_plotter import twoD_plotter, PCA_plotter, get_tSNE_embedding, get_PCA_embedding
from poc_features import retrieve_hash_with_cwe, dict_with_count, retrieve_hash_with_author
from litmus_test import create_dataset, create_dataset_multiclass, get_embedding_for_dataset, get_embedding_for_dataset_multiclass, train_test_linear_classifier



def read_json(file_name):
    docs = []
    with open(file_name) as f:
        for json_obj in f:
            data = json.loads(json_obj)
            docs.append(data)
    return docs


def retrieve_PL_codes(path, lang='py'):
    os.chdir(path)
    codes_list_w_functions = []
    file_names = []
    for file in os.listdir():
        file_path = f"{path}/{file}"
        extension = (str(file)).split('.')[1]
        #print(extension)
        if (extension == lang):
            #print(extension)
            #print(file_path)
            data = read_json(file_path)
            code_list = (data[0]['noncomments'])
            if (len(code_list) > 0):
                # print(code_list)
                code_lines = ""
                # code_tokens = [tokenizer.cls_token]
                for code_line in code_list:
                    code_lines += code_line

                functions_list = code_to_function(code_lines)
                codes_list_w_functions.append(functions_list)
                file_names.append(file)

    return codes_list_w_functions, file_names


if __name__ == '__main__':

    model, tokenizer = load_model_tokenizer()

    path = r"/Users/shoumik/Desktop/UMD 1st semester/Research/Dataset/copy20221006/files/train_19700101_20210401/exploits_text"
    common_path = "/Users/shoumik/Desktop/UMD 1st semester/Research/Dataset/copy20221006/files/train_19700101_20210401/exploits_function/"

    codes_list_w_functions, file_names = retrieve_PL_codes(path)
    print(len(codes_list_w_functions))
    print(len(file_names))

    """
    function_count = 0
    function_within_threshold_count = 0

    for i, code in enumerate(codes_list_w_functions):
        file = open(common_path+file_names[i], "w")
        print(file_names[i])
        print("Number of functions: ", len(code))
        file.write("Number of functions: " + str(len(code)) + "\n")
        function_count += len(code)
        for j, function in enumerate(code):
            #print(j+1, ") Length of function: ", len(function))
            pl_tokens = tokenizer.tokenize(function)
            print(len(pl_tokens))
            if(len(pl_tokens)<510): function_within_threshold_count += 1
            file.write(str(len(pl_tokens)))
            file.write("\n")
            file.writelines(function)
        file.close()
    # print(codes_list_w_functions)

    print(function_count, function_within_threshold_count)
    """
    """
    POC_embeddings = []
    for i, code in enumerate(codes_list_w_functions):
        for j, function in enumerate(code):
            function_embedding = get_embedding_from_input(model, tokenizer, function)
            POC_embeddings.append(function_embedding.detach().numpy())
            #print(function_embedding.shape)
            #print(function_embedding)
        #if(i==5):  break
    POC_embeddings = np.array(POC_embeddings)
    print(POC_embeddings.shape)
    #print(POC_embeddings)
    file = open("/Users/shoumik/Desktop/UMD 1st semester/Research/Code/CodeBert on Exploits/embeddings/poc_embedding.npy", 'wb')
    np.save(file, POC_embeddings)
    file.close()
    """

    """
    path = "/Users/shoumik/Desktop/UMD 1st semester/Research/Dataset/CodeSearchNet 2/python/test.jsonl"
    NON_POC_function_list = convert_json_file_to_list_of_functions(path)
    #print(len(NON_POC_function_list))
    #print(len(NON_POC_function_list[0]))
    NON_POC_embeddings = []
    for i, function in enumerate(NON_POC_function_list):
        function_embedding = get_embedding_from_input(model, tokenizer, function)
        NON_POC_embeddings.append(function_embedding.detach().numpy())
        if(i==4500):   break
    NON_POC_embeddings = np.array(NON_POC_embeddings)
    print(NON_POC_embeddings.shape)
    file = open("/Users/shoumik/Desktop/UMD 1st semester/Research/Code/CodeBert on Exploits/embeddings/non_poc_embedding.npy", 'wb')
    np.save(file, NON_POC_embeddings)
    file.close()
    """

    common_path_embedding = "/Users/shoumik/Desktop/UMD 1st semester/Research/Code/CodeBert on Exploits/embeddings/"

    #POC_embeddings = np.load(common_path_embedding + "poc_embedding.npy")
    #print(POC_embeddings.shape)

    #non_POC_embeddings = np.load(common_path_embedding + "non_poc_embedding.npy")
    #print(non_POC_embeddings.shape)

    """
    combined_embedding = np.concatenate((POC_embeddings, non_POC_embeddings))
    print(combined_embedding.shape)
    combined_embedding_CLS = combined_embedding[:, :, 0, :]
    print(combined_embedding_CLS.shape)
    combined_embedding_CLS = np.reshape(combined_embedding_CLS, (combined_embedding_CLS.shape[0], combined_embedding_CLS.shape[2]))
    #combined_embedding = np.reshape(combined_embedding, (combined_embedding.shape[0], combined_embedding.shape[2]*combined_embedding.shape[3]))
    print(combined_embedding_CLS.shape)
    del POC_embeddings
    del non_POC_embeddings
    del combined_embedding

    y_PoC = np.ones(4475)
    y_non_PoC = np.zeros(4501)
    y = np.concatenate((y_PoC, y_non_PoC))
    print(y.shape)

    PCA_embedding = get_PCA_embedding(2, combined_embedding_CLS)
    tSNE_plotter(PCA_embedding, y, 'PoC vs non-PoC functions CLS PCA projection')
    print(PCA_embedding.shape)
    """

    #tSNE_embedding = get_tSNE_embedding(2, PCA_embedding)
    #print(tSNE_embedding.shape)
    #tSNE_plotter(tSNE_embedding, y, 'PoC vs non-PoC functions CLS t-SNE projection')

    """
    PCA_embedding = np.load(common_path_embedding + 'PCA_embedding.npy')
    tsne_embedding = np.load(common_path_embedding + 'tSNE_embedding.npy')
    

    tSNE_plotter(tsne_embedding, y)
    PCA_plotter(PCA_embedding, y)
    """

    path = "/Users/shoumik/Desktop/UMD 1st semester/Research/Dataset/copy20221006/files/train_19700101_20210401/"
    """
    py_hash_cwe_file_path = path + "py_cve_cwe.csv"
    hashes, cwes = retrieve_hash_with_cwe(py_hash_cwe_file_path)
    print(hashes, cwes)
    cwe_count_dict = dict_with_count(cwes)
    print(cwe_count_dict)

    """
    py_hash_author = path + "py_author.csv"
    hashes, authors = retrieve_hash_with_author(py_hash_author)
    print(hashes, authors)
    author_count_dict = dict_with_count(authors)
    print(author_count_dict)




    #x, y = create_dataset_multiclass(codes_list_w_functions, file_names, hashes, authors, [' His0k4', ' Abysssec', ' muts'])
    x, y = create_dataset_multiclass(codes_list_w_functions, file_names, hashes, authors, [' His0k4', ' Abysssec', ' muts', ' mr_me', ' loneferret', ' shinnai', ' Chris Lyne'])
    print(x, y)
    """
    print(len(pos_x), len(neg_x))
    pos_embs, neg_embs = get_embedding_for_dataset(model, tokenizer, pos_x, neg_x)
    del pos_x, neg_x
    print(pos_embs.shape, neg_embs.shape)
    y1 = np.ones(pos_embs.shape[0])
    y2 = np.zeros(neg_embs.shape[0])
    y = np.concatenate((y1, y2))
    print(y.shape)
    combined_embs = np.concatenate((pos_embs, neg_embs))
    """
    #file = open("/Users/shoumik/Desktop/UMD 1st semester/Research/Code/CodeBert on Exploits/embeddings/cwe78_embs.npy", 'wb')
    #np.save(file, pos_embs)
    #file = open("/Users/shoumik/Desktop/UMD 1st semester/Research/Code/CodeBert on Exploits/embeddings/cwe89_embs.npy", 'wb')
    #np.save(file, neg_embs)
    embs_multiclass, y_list = get_embedding_for_dataset_multiclass(model, tokenizer, x, y)
    print(embs_multiclass, y_list)
    embs_multiclass = embs_multiclass[:, :, 0, :]
    embs_multiclass = np.reshape(embs_multiclass, (embs_multiclass.shape[0], embs_multiclass.shape[2]))
    print(embs_multiclass.shape)
    """
    del pos_embs, neg_embs
    print(combined_embs.shape)
    combined_embs = combined_embs[:, :, 0, :]
    combined_embs = np.reshape(combined_embs, (combined_embs.shape[0], combined_embs.shape[2]))
    PCA_embs = get_PCA_embedding(2, combined_embs)
    twoD_plotter(PCA_embs, y, "muts vs Chris Lyne")
    """
    #tsne_embs = get_tSNE_embedding(2, combined_embs)
    #twoD_plotter(tsne_embs, y, "tSNE CWE-434 vs CWE-89")
    train_test_linear_classifier(embs_multiclass, y_list)
    del embs_multiclass


