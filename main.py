

# Press ⌃R to execute it or replace it with your code.
# Press Double ⇧ to search everywhere for classes, files, tool windows, actions, and settings.

import torch
from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, AutoTokenizer, AutoModel

import json
import os
from code_processor import code_to_function


def read_json(file_name):
    docs = []
    with open(file_name) as f:
        for json_obj in f:
            data = json.loads(json_obj)
            docs.append(data)
    return docs


def retrieve_PL_codes(path, lang='py'):
    os.chdir(path)
    codes_list_w_functions = []
    file_names = []
    for file in os.listdir():
        file_path = f"{path}/{file}"
        extension = (str(file)).split('.')[1]
        #print(extension)
        if (extension == lang):
            #print(extension)
            #print(file_path)
            data = read_json(file_path)
            code_list = (data[0]['noncomments'])
            if (len(code_list) > 0):
                # print(code_list)
                code_lines = ""
                # code_tokens = [tokenizer.cls_token]
                for code_line in code_list:
                    code_lines += code_line

                functions_list = code_to_function(code_lines)
                codes_list_w_functions.append(functions_list)
                file_names.append(file)

    return codes_list_w_functions, file_names


if __name__ == '__main__':
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # tokenizer = RobertaTokenizer.from_pretrained("microsoft/codebert-base")
    # model = RobertaModel.from_pretrained("microsoft/codebert-base")
    tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
    model = AutoModel.from_pretrained("microsoft/codebert-base")
    model.to(device)
    # print(model)

    path = r"/Users/shoumik/Desktop/UMD 1st semester/Research/Dataset/copy20221006/files/train_19700101_20210401/exploits_text"
    common_path = "/Users/shoumik/Desktop/UMD 1st semester/Research/Dataset/copy20221006/files/train_19700101_20210401/exploits_function/"

    codes_list_w_functions, file_names = retrieve_PL_codes(path)
    print(len(codes_list_w_functions))


    for i, code in enumerate(codes_list_w_functions):
        file = open(common_path+file_names[i], "w")
        print(file_names[i])
        print("Number of functions: ", len(code))
        file.write("Number of functions: " + str(len(code)) + "\n")
        for j, function in enumerate(code):
            #print(j+1, ") Length of function: ", len(function))
            pl_tokens = tokenizer.tokenize(function)
            print(len(pl_tokens))
            file.write(str(len(pl_tokens)))
            file.write("\n")
            file.writelines(function)
        file.close()
    # print(codes_list_w_functions)
    """
    code_tokens = tokenizer.tokenize(codes_list_w_functions[0][1])
    code_tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.eos_token]
    print(code_tokens)
    tokens_ids = tokenizer.convert_tokens_to_ids(code_tokens)
    print(len(tokens_ids))
    context_embeddings = model(torch.tensor(tokens_ids)[None, :])[0]
    print(codes_list_w_functions[0][1])
    print(context_embeddings)
    print(context_embeddings.shape)
    """

