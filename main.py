

# Press ⌃R to execute it or replace it with your code.
# Press Double ⇧ to search everywhere for classes, files, tool windows, actions, and settings.

import torch
from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, AutoTokenizer, AutoModel

import json
import os
from code_processor import code_to_function

from code_to_embedding import load_model_tokenizer, get_embedding_from_input


def read_json(file_name):
    docs = []
    with open(file_name) as f:
        for json_obj in f:
            data = json.loads(json_obj)
            docs.append(data)
    return docs


def retrieve_PL_codes(path, lang='py'):
    os.chdir(path)
    codes_list_w_functions = []
    file_names = []
    for file in os.listdir():
        file_path = f"{path}/{file}"
        extension = (str(file)).split('.')[1]
        #print(extension)
        if (extension == lang):
            #print(extension)
            #print(file_path)
            data = read_json(file_path)
            code_list = (data[0]['noncomments'])
            if (len(code_list) > 0):
                # print(code_list)
                code_lines = ""
                # code_tokens = [tokenizer.cls_token]
                for code_line in code_list:
                    code_lines += code_line

                functions_list = code_to_function(code_lines)
                codes_list_w_functions.append(functions_list)
                file_names.append(file)

    return codes_list_w_functions, file_names


if __name__ == '__main__':

    model, tokenizer = load_model_tokenizer()

    path = r"/Users/shoumik/Desktop/UMD 1st semester/Research/Dataset/copy20221006/files/train_19700101_20210401/exploits_text"
    common_path = "/Users/shoumik/Desktop/UMD 1st semester/Research/Dataset/copy20221006/files/train_19700101_20210401/exploits_function/"

    codes_list_w_functions, file_names = retrieve_PL_codes(path)
    print(len(codes_list_w_functions))

    """
    function_count = 0
    function_within_threshold_count = 0

    for i, code in enumerate(codes_list_w_functions):
        file = open(common_path+file_names[i], "w")
        print(file_names[i])
        print("Number of functions: ", len(code))
        file.write("Number of functions: " + str(len(code)) + "\n")
        function_count += len(code)
        for j, function in enumerate(code):
            #print(j+1, ") Length of function: ", len(function))
            pl_tokens = tokenizer.tokenize(function)
            print(len(pl_tokens))
            if(len(pl_tokens)<510): function_within_threshold_count += 1
            file.write(str(len(pl_tokens)))
            file.write("\n")
            file.writelines(function)
        file.close()
    # print(codes_list_w_functions)

    print(function_count, function_within_threshold_count)
    """
    for i, code in enumerate(codes_list_w_functions):
        for j, function in enumerate(code):
            function_embedding = get_embedding_from_input(model, tokenizer, function)
            print(function_embedding.shape)
            print(function_embedding)
