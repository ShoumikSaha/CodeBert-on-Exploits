import tokenize
from io import BytesIO
from collections import deque


def code_to_function(code_string):
    functions = []
    code_string.strip()
    file = BytesIO(code_string.encode())
    try:
        tokens = deque(tokenize.tokenize(file.readline))
    except:
        return functions
    lines = []
    #print(tokens)
    non_function_string = ""
    while tokens:
        token = tokens.popleft()
        #print(token)
        if token.type == tokenize.NAME and token.string == 'def':
            function = token.string
            start_line, _ = token.start
            last_token = token
            while tokens:
                token = tokens.popleft()
                if token.type == tokenize.NEWLINE:
                    break
                last_token = token
                function += " " + last_token.string
            if last_token.type == tokenize.OP and last_token.string == ':':
                indents = 0
                while tokens:
                    token = tokens.popleft()
                    if token.type == tokenize.NL:
                        continue
                    if token.type == tokenize.INDENT:
                        indents += 1
                    elif token.type == tokenize.DEDENT:
                        indents -= 1
                        if not indents:
                            break
                    else:
                        last_token = token
                    function += " " + last_token.string
            lines.append((start_line, last_token.end[0]))
            functions.append(function)
        else:
            if token.type == tokenize.NL:
                continue
            else:
                non_function_string += " " + token.string
    """
    for f in functions:
        print(len(f))
        print('-' * 40)
        print(f)
    """
    #print("Non-function string: ", non_function_string)
    functions.append(non_function_string)
    return functions
    #print(code_string[0:15])
