import tokenize
from io import BytesIO
from collections import deque

def truncate_if_long_string(token, threshold=10):
    """
    If the value of a string in code exceeds the threshold, then it just takes up to the threshold
    :param token:
    :param threshold:
    :return: string token that is smaller than the threshold
    """
    if(token.type == tokenize.STRING):
        token_string = token.string
        if len(token_string) > threshold:
            return token_string[:threshold] + "\""

    return token.string


def code_to_function(code_string):
    """
    Divides a code into multiple functions
    :param code_string: string containing the whole code
    :return: a list of functions in the code
    """
    functions = []
    code_string.strip()
    file = BytesIO(code_string.encode())
    try:
        tokens = deque(tokenize.tokenize(file.readline))
    except:
        return functions
    lines = []
    #print(tokens)
    non_function_string = ""
    while tokens:
        token = tokens.popleft()
        #print(token)
        if token.type == tokenize.NAME and token.string == 'def':
            function = token.string
            start_line, _ = token.start
            last_token = token
            while tokens:
                token = tokens.popleft()
                if token.type == tokenize.NEWLINE:
                    break
                last_token = token
                function += " " + truncate_if_long_string(last_token)
            if last_token.type == tokenize.OP and last_token.string == ':':
                indents = 0
                while tokens:
                    token = tokens.popleft()
                    if token.type == tokenize.NL:
                        continue
                    if token.type == tokenize.INDENT:
                        indents += 1
                    elif token.type == tokenize.DEDENT:
                        indents -= 1
                        if not indents:
                            break
                    else:
                        last_token = token
                    function += " " + truncate_if_long_string(last_token)
            lines.append((start_line, last_token.end[0]))
            functions.append(function)
        else:
            if token.type == tokenize.NL:
                continue
            else:
                non_function_string += " " + truncate_if_long_string(token)
    """
    for f in functions:
        print(len(f))
        print('-' * 40)
        print(f)
    """
    #print("Non-function string: ", non_function_string)
    functions.append(non_function_string)
    return functions
    #print(code_string[0:15])
