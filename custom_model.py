import numpy as np
import torch
from torch import nn
from code_to_embedding import load_model_tokenizer, get_embedding_from_input
from transformers import LongformerModel, RobertaTokenizer
from litmus_test import train_test_linear_classifier
from sklearn.model_selection import train_test_split


def encoder_section(input_functions, num_encoders=10):
    embedding_from_encoders = []
    #print(len(input_functions))
    for i in range(len(input_functions)):
        model, tokenizer = load_model_tokenizer()
        embedding = get_embedding_from_input(model, tokenizer, input_functions[i])
        embedding_from_encoders.append(embedding.detach().numpy())
    embedding_from_encoders = np.asarray(embedding_from_encoders)
    embedding_from_encoders = embedding_from_encoders[0:num_encoders, :, 0, :]
    #embedding_from_encoders = np.reshape(embedding_from_encoders, (embedding_from_encoders.shape[0], embedding_from_encoders.shape[2]))
    embedding_from_encoders = np.reshape(embedding_from_encoders, -1)
    embedding_from_encoders = np.pad(embedding_from_encoders, (0, num_encoders*768-embedding_from_encoders.shape[0]))
    #print(embedding_from_encoders)
    #print(embedding_from_encoders.shape)
    #print(embedding_from_encoders)
    return embedding_from_encoders

def custom_model_with_SGDclassifier(input_codes, labels, num_encoders):
    embedding_data = []
    for i, code in enumerate(input_codes):
        embedding_from_encoders = encoder_section(code, num_encoders)
        embedding_data.append(embedding_from_encoders)
        #if(i==9): break

    embedding_data = np.asarray(embedding_data)
    print(embedding_data.shape)
    labels = np.asarray(labels)
    print(labels.shape)
    train_test_linear_classifier(embedding_data, labels)


class NeuralNetwork(nn.Module):
    def __init__(self, num_encoders):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(num_encoders*768, 512),
            nn.ReLU(),
            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Sigmoid(),
        )

    def forward(self, x):
        #x = self.flatten(x)
        out = self.linear_relu_stack(x)
        return out
def train_neural_model(model, data_x, data_y, epochs=1000):
    loss_function = nn.MSELoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

    for epoch in range(epochs):
        pred_y = model(data_x)
        #print(pred_y)
        #print(data_y)
        loss = loss_function(pred_y, data_y)
        #losses.append(loss.item())

        model.zero_grad()
        loss.backward()
        optimizer.step()

    return model

def evaluate_neural_model(model, data_x, data_y):
    predicted = model(data_x)
    data_y = data_y.reshape(-1).detach().numpy()
    acc = (predicted.reshape(-1).detach().numpy().round() == data_y).mean()
    return acc
def custom_model_with_neural_net(input_codes, labels, num_encoders):
    embedding_data = []
    for i, code in enumerate(input_codes):
        embedding_from_encoders = encoder_section(code, num_encoders)
        embedding_data.append(embedding_from_encoders)
        if(i==9): break
    embedding_data = np.asarray(embedding_data)
    embedding_data = torch.from_numpy(embedding_data)
    print(embedding_data.shape)
    labels = np.asarray(labels[0:10])
    labels = np.reshape(labels, (-1, 1))
    labels = torch.tensor(labels, dtype=torch.float)
    print(labels.shape)

    X_train, X_test, y_train, y_test = train_test_split(embedding_data, labels, test_size=0.2, random_state=10)

    neural_model = NeuralNetwork(num_encoders)
    print(neural_model)
    neural_model = train_neural_model(neural_model, X_train, y_train)

    print(evaluate_neural_model(neural_model, X_train, y_train))
    print(evaluate_neural_model(neural_model, X_test, y_test))


