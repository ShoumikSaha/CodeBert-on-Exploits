import torch
from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, AutoTokenizer, AutoModel


def load_model_tokenizer():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # tokenizer = RobertaTokenizer.from_pretrained("microsoft/codebert-base")
    # model = RobertaModel.from_pretrained("microsoft/codebert-base")
    tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
    model = AutoModel.from_pretrained("microsoft/codebert-base")
    model.to(device)
    # print(model)
    return model, tokenizer


def get_embedding_from_input(model, tokenizer, function):
    pl_tokens = tokenizer.tokenize(function)
    pl_tokens = [tokenizer.cls_token] + pl_tokens + [tokenizer.eos_token]
    if(len(pl_tokens)>512):
        return
    else:
        tokens_ids = tokenizer.convert_tokens_to_ids(pl_tokens)
        context_embeddings = model(torch.tensor(tokens_ids)[None, :])[0]
        return context_embeddings