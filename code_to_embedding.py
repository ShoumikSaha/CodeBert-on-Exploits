import torch
from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, AutoTokenizer, AutoModel


def load_model_tokenizer(model_name = "microsoft/codebert-base"):
    """

    :param model_name: string that defines the model name
    :return: model and tokenizer object
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # tokenizer = RobertaTokenizer.from_pretrained("microsoft/codebert-base")
    # model = RobertaModel.from_pretrained("microsoft/codebert-base")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    model.to(device)
    # print(model)
    return model, tokenizer


def get_embedding_from_input(model, tokenizer, function, max_length=512):
    """

    :param model:
    :param tokenizer:
    :param function: an individual function (can get cut due to the length limit)
    :return: [CLS] embedding for the function
    """
    pl_tokens = tokenizer.tokenize(function, padding="max_length", max_length=max_length, truncation=True)
    #pl_tokens = [tokenizer.cls_token] + pl_tokens + [tokenizer.eos_token]

    tokens_ids = tokenizer.convert_tokens_to_ids(pl_tokens)
    context_embeddings = model(torch.tensor(tokens_ids)[None, :])[0]
    return context_embeddings

